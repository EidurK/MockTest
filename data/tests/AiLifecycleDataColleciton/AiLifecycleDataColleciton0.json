[{"question": "Imagine you are developing a machine learning pipeline that involves both data collection and feature engineering. Which of the following practices is most critical to ensure that the model's performance assessment is reliable and not overly optimistic due to data leakage?", "options": ["Using data augmentation on the training set to increase its size as much as possible.", "Implementing rigorous data partitioning to clearly separate training, validation, and test sets before any feature engineering is performed.", "Applying random sampling exclusively to create the largest possible training dataset.", "Relying on imbalanced data to naturally reflect real-world distributions without any correction."]}, {"question": "Consider a system that collects data from users interacting with various features. Which of the following best describes 'interaction data' and explains the potential risk known as 'analytics debt' when such data is not effectively used?", "options": ["Interaction data is information gathered from user interactions including context, actions, and outcomes. Failing to leverage this data may lead to analytics debt, where valuable insights are missed, potentially reducing the effectiveness of personalized features like search result reranking.", "Interaction data consists solely of the final outcomes of user activities, and analytics debt refers to the backlog of system updates caused by excessive data processing.", "Interaction data involves only user click patterns without contextual details, and analytics debt means the extra cost incurred when integrating additional marketing data.", "Interaction data is limited to the timestamps of user actions, and analytics debt is the challenge of storing large volumes of time-series data leading to slower system performance."]}, {"question": "Imagine you are developing a machine learning model to predict house prices. During your analysis, you notice one of your features is 'price per square footage', which inadvertently includes data closely related to past house prices (the target). Which statement best describes this issue and a viable strategy to prevent it?", "options": ["This is an example of data leakage\u2014information that would not be available at prediction time is included in training. A viable prevention strategy is to simulate real-world conditions during validation, such as using a chronological split of data.", "This scenario indicates overfitting, where the model learns noise from the training data. The solution is to collect more data to generalize better.", "The issue is underfitting, meaning the model is too simple to capture the pattern. A remedy is to add more features irrespective of their relevance.", "This situation describes improper hyperparameter tuning due to including all available features in training. The fix is to use grid search for hyperparameter optimization."]}, {"question": "Imagine you are working with a dataset that contains time-series data and several groups of related observations. Which of the following approaches best adheres to the best practices for data partitioning to both assess model performance and prevent overfitting?", "options": ["A) Preprocess the raw dataset entirely, then randomly shuffle and split it into training, validation, and test sets.", "B) Randomly shuffle the raw data and split it into groups, even if that might break inherent temporal or group relationships.", "C) Partition the raw data into training, validation, and test sets before any preprocessing, ensuring that the temporal and group relationships are preserved, and then process each subset independently.", "D) Perform all preprocessing steps on the raw data, split afterward while preserving group relationships by using stratification on non-temporal attributes."]}, {"question": "When handling missing data in a machine learning project, which approach is best for preventing data leakage and maintaining model validity?", "options": ["Use imputation on the training data only (calculating means, medians, etc.) and then apply the same parameters to the test data.", "Perform imputation individually on both training and test datasets to suit each set\u2019s distribution.", "Remove rows with missing values exclusively from the test data to avoid contaminating model training.", "Generate binary flags for missing data only in the test set to highlight absent information without modifying the training set."]}, {"question": "Consider a scenario where you need to augment your image and text datasets to improve a machine learning model while ensuring that semantic integrity and contextual relevance are preserved. Which of the following approaches correctly applies data augmentation techniques for both images and text?", "options": ["For images, apply rotation, translation, and scaling; for text, perform synonym replacement, back translation, and deletion using BERT predictions.", "For images, adjust the pixel intensity randomly; for text, convert all text to uppercase.", "For images, add random noise without geometric transformations; for text, randomly shuffle words in each sentence.", "For images, only apply cropping; for text, exclusively replace words with their hypernyms without checking context."]}, {"question": "A health analytics firm is developing a model to predict rare disease occurrences. The dataset is highly imbalanced, where the rare disease constitutes only 10% of the cases, leading the model to favor predictions for the majority class. Which of the following strategies best addresses the imbalance while taking into account the risk of overfitting and potential loss of information?", "options": ["Utilize oversampling techniques such as SMOTE or ADASYN to synthetically augment the minority class samples.", "Completely remove the majority class samples to balance the dataset.", "Collect additional data by exclusively focusing on the majority class to strengthen model accuracy.", "Implement a simpler model architecture without any data sampling methods."]}, {"question": "Imagine you are tasked with preparing data for a classification model in which the classes are unevenly represented. Which random sampling technique should you use to ensure that the proportions of each class in your sample are similar to those in the overall dataset, and why?", "options": ["Simple Random Sampling, because it gives an equal chance to each data point without considering class distribution.", "Stratified Random Sampling, because it divides the dataset into classes and samples proportionally from each, ensuring representation.", "Cluster-Based Sampling, because it groups data points and randomly selects whole clusters, assuming clusters represent class distributions.", "Systematic Sampling, because it selects every nth data point, which inherently preserves the class distribution."]}, {"question": "Imagine a scenario where a data engineering team is storing a mix of unstructured data and code. They use object storage (e.g., Amazon S3) to manage unstructured data and a small codebase, but they also handle very large datasets that change over time and must be tracked for reproducibility and potential rollbacks. Which of the following approaches best addresses the needs for versioning the large datasets while managing the code and small datasets effectively?", "options": ["Use joint versioning with Git or Git LFS for both the code and the large datasets, ensuring consistency in version control.", "Leverage specialized data versioning tools (like Data Version Control or Pachyderm) for large datasets while storing and versioning the code and small assets using Git.", "Rely solely on the versioning features provided by the object storage platform (e.g., Amazon S3) to automatically track all changes in the large datasets.", "Separate large datasets completely from any version control system to improve performance, while using manual backups for the code."]}, {"question": "Imagine you are preparing a dataset for a new machine learning project. Which approach best applies the general best practices for data management, as described in the text?", "options": ["Automate data transformation to reduce errors, thoroughly document the data schema, collection methods, pre-processing steps, exclusions, file formats, and assign clear responsibilities, while prioritizing data quality over quantity.", "Manually transform data to maintain hands-on control, focusing on acquiring a large volume of data and adding documentation only if time permits.", "Rely on sporadic manual updates by team members for data transformation, prioritizing quantity over quality, and update documentation only after the project is complete.", "Automate data processing without maintaining documentation to speed up development, and assume that data quality will be addressed during later stages."]}]